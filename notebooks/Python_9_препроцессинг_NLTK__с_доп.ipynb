{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xKMFDWff6OJ9",
        "JTrq8OAKblkU",
        "vPvYUki89ZHp",
        "75EnFxyh9CNo",
        "HH5xf4oiGOHY",
        "H4nIAbE9HRao",
        "ZPmJIpx9HnhQ",
        "u7oNBTGlINjJ",
        "FrbsmxrIJNsx",
        "H4bmxuREioDi",
        "WGADmwFneCgr",
        "V8HSb8QdPwZU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Препроцессинг NLTK\n",
        "\n",
        "Подготовка текста для анализа\n",
        "\n",
        "<s>Ведь мы c вами все знаем, что на самом деле цифровой анализ текста - это и есть частотности слов</s> :)\n",
        "\n",
        "Используются материалы из тетрадок Д.Скоринкина, А. Хорошевой"
      ],
      "metadata": {
        "id": "xKMFDWff6OJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQfRhCxL3yA1",
        "outputId": "67c4880d-4267-45e9-e3ce-551d77a2626a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "давайте нормализуем этот текст\n"
          ]
        }
      ],
      "source": [
        "# кое-что мы уже умеем, например: \n",
        "\n",
        "print(\" Давайте нормализуем этот текст!      \".lower().strip(\" )?!.\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# функция, убирающая пунктуацию\n",
        "\n",
        "import string\n",
        "\n",
        "def normalize(text):\n",
        "    normalized = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
        "    return normalized\n",
        "\n",
        "text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\"\n",
        "\n",
        "print(normalize(text))\n",
        "print(normalize('Привет, здравствуй мир!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL8oD4Rm6uR5",
        "outputId": "b3c38232-36a1-4ee1-dc4b-3694308dad12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn’t', 'hold', 'with', 'such', 'nonsense']\n",
            "['привет', 'здравствуй', 'мир']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод .maketrans() имеет три аргумента и создает таблицу перевода:\n",
        "\n",
        "* какие символы переводить (первый аргумент)\n",
        "* в какие переводить (второй аргумент)\n",
        "* какие символы удалять (третий аргумент)\n",
        "Метод .translate() использует таблицу перевода, чтобы превратить символы в новые символы.\n",
        "\n",
        "В нашем случае, в методе .maketrans() первые два аргумента мы оставляем пустыми (ничто переводится в ничто), а аргумент для удаления определяем как строку punctuation, в которой содержатся все символы пунктуации."
      ],
      "metadata": {
        "id": "pZ3Pm9ey8Qgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Оставим на память об идее тройных кавычек :)"
      ],
      "metadata": {
        "id": "JTrq8OAKblkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = \"Hello,\\nworld\"\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDu6BtG9gpg4",
        "outputId": "38659ee6-11c3-4721-dbdc-38789775216a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,\n",
            "world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = \"\"\"Пишу так, как хочу\n",
        "\n",
        "\n",
        "Мой текст еще не закончился\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Конец\"\"\"\n",
        "print(p2)\n",
        "# если мы записываем текст с тройными кавычками, не используем \\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IclnAgXzg0js",
        "outputId": "dd269875-6fab-45d6-d6e0-cae159c103df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пишу так, как хочу\n",
            "\n",
            "\n",
            "Мой текст еще не закончился\n",
            "\n",
            "\n",
            "Конец\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZCp9S9MwhsPh",
        "outputId": "eb533c0b-1150-43d6-9a79-3d2b8117ab34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Пишу так, как хочу\\n\\n\\nМой текст еще не закончился\\n\\n\\nКонец'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "print(re.findall(r'\\n', p2)) # raw-строки, сырые строки, не работает экранирование"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHyIUrqCiJ9K",
        "outputId": "ccbc2bdf-5896-447d-dfcd-68ae1d1fc64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задача 1: вспомним то, что уже умеем"
      ],
      "metadata": {
        "id": "vPvYUki89ZHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем лучшего писателя отзывов. \n",
        "У вас есть четрые отзыва на фильм С.Кубрика \"Сияние\". Вам нужно определить, кто из авторов отзывов написал отзыв с наибольшим количеством уникальных слов.\n",
        "    \n",
        "    Для определения вам нужно: (для каждого автора)\n",
        "\n",
        "    1. Предобработать строку, сведя все к нижнему регистру, убрать пунктуацию.\n",
        "    2. Превратить строку в список \n",
        "    3. Оставить уникальные элементы в списке (превратить список во множество)\n",
        "    4. Определить размер такого множества\n",
        "    \n",
        "У кого из авторов количество уникальных слов наибольшее?\n"
      ],
      "metadata": {
        "id": "1Yf1XqpY-Cdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paul = \"\"\"When this film first came out in 1980, I remember going to see it on opening night. This movie just scared the life out of me, which is what still happens every time\n",
        "I rent the video for a re-watch.I have seen The Shining at least six or seven times, and I still believe it to be simultaneously and paradoxically one of the most frightening and yet funniest films I've ever seen. Frightening because of the extraordinarily effective use of long shots to create feelings of isolation, convex lens shots to enhance surrealism, and meticulously scored music to bring tension levels to virtually unbearable levels. And \\\"funny\\\" because of Jack Nicholson's outrageous and in many cases ad-libbed onscreen antics. It never ceases to amaze me how The Shining is actually two films in one, both a comedy AND a horror flick. Ghostly apparitions of a strikingly menacing nature haunt much of the first half of the film, which gradually evolve into ever more serious physical threats as time progresses. Be that as it may, there is surprisingly little violence given the apparent intensity, but that is little comfort \n",
        "for the feint of heart as much of the terror is more implied than manifest. The Shining is a truly frightening movie that works symbolically on many levels, but is basically  about human shortcomings and the way they can be exploited by unconscious forces combined with weakness of will. This film scares the most just by using suggestion to turn your own imagination against you. The Shining is a brilliant cinematic masterpiece, the likes of which have never been seen before or since. Highly, highly recommended.\"\"\"\n",
        "\n",
        "jane = \"\"\"Chilling, majestic piece of cinematic fright, this film combines all the great elements of an intellectual thriller, with the grand vision of a director who has the instinctual capacity to pace a moody horror flick within the realm of his filmmaking genius that includes an eye for the original shot, an ice-cold soundtrack and an overall sense of dehumanization. This movie cuts through all the typical horror\n",
        "movies like a red-poker through a human eye, as it allows the viewer to not only feel the violence and psychosis of its protagonist, but appreciate the seed from which the derangement stems. One of the scariest things for people\n",
        "to face is the unknown and this film presents its plotting with just that thought in mind. The setting is perfect, in a desolate winter hideaway. The quietness of the moment is a character in itself, as the fermenting aggressor in Jack Torrance's mind wallows in this idle time, and breeds the devil's new playground. I always felt like the presence of evil was dormant in all of our minds, with only the circumstances of the moment, and the reasons given therein, needed to wake its violent ass and pounce over its unsuspecting victims. This film is a perfect example of this very thought.\"\"\"\n",
        "\n",
        "kate = \"\"\"What can I say about the scariest movie I have ever seen that has not already been said by others more articulate than yours truly? Do not view this film expecting to see a screen version of the Stephen King novel.\n",
        "Rather, this is a Stanley Kubrick film, and to fully appreciate it one should judge it within the context of Kubrick's entire body of work as a serious filmmaker. Thematically, THE SHINING relates most closely to 2001:\n",
        "A SPACE ODYSSEY, though flourishes of PATHS OF GLORY, A CLOCKWORK ORANGE and BARRY LYNDON do manage to figure prominently in the film's overall technique. In a nutshell (no pun intended), Jack Nicholson and Shelly Duvall co-star with Oregon's Timberline Lodge - enlisted to portray the exterior of the Overlook Hotel - in a story that appears on the surface to be about ghosts and insanity, but deals with issues of child abuse, \n",
        "immortality and duality. What the film might lack initially in terms of coherence is more than made up for in technique. Garrett Brown (the male voice in those old Molson Golden commercials), inventor of the Steadicam,chases young Danny Lloyd through hotel corridors and an amazing snow maze, providing magic-carpet-ride fluidity to scenes that ten years earlier would have been impossible to accomplish. If the film starts off too slow, remember who the director is. This man likes to take his time, and the results are well worth it: incredible aerial shots of the Overlook Hotel; horrific Diane Arbus-inspired twins staring directly at us; portentous room 237 and its treasure trove of terrible secrets; elevators that gush rivers of blood in slow-motion; Jack Torrance's immortality found via the hotel (akin to David Bowman's journey through the Space Gate); and some of the best use of pre-existing music ever assembled for a motion picture.\"\"\"\n",
        "\n",
        "nick = \"\"\"I was never a big fan of horror movies. They usually try cheap tricks to scare their audiences like loud noises and creepy children. They usually lack originality and contain overacting galore. The only horror movie i\n",
        "like was Stir of Echoes with Kevin Bacon. It was well-acted, and had a great story. But it has been joined and maybe even surpassed by Stanley Kubrick's The Shining, quite possibly the scariest movie ever. The movie follows a writer (Jack Nicholson) and his family who agree to watch over a hotel while it is closed for the winter. There were rumors of the place being haunted and the last resident went crazy and murdered his family. But Jack is convinced it will be OK and he can use the quiet to overcome his writer's block. After months of solitude and silence however, Jack becomes a grumpy and later violent. Is it cabin fever or is there something in the hotel that is driving him mad? One of the creepiest parts about the movie is the feeling of isolation that Kubrick makes. The hotel is very silent, and the rooms are huge, yet always empty. It is also eerily calm when Jack's son is riding his bike through the barren hallways. Jack Nicholson's performance is also one of his very best, scaring the hell out of me and making me sure to get out once in awhile. My favorite scene is when he is talking to a ghost from inside a walk-in refrigerator. The Shining is tops for horror movies in my opinion, beating the snot out of crap like the Ring and The Blair Witch Project. It may be a oldie, but is definitely a goodie.\"\"\""
      ],
      "metadata": {
        "id": "sM8a9Tlw-AEu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# мы можем использовать тройные кавычки (то есть три одинарные кавычки или три двойные кавычки) для строк с одинарными и двойными кавычками\n",
        "# так мы исключаем необходимость экранирования любых кавычек\n",
        "# а еще можем записывать перенос строки без экранирования \\n"
      ],
      "metadata": {
        "id": "12FjlgZz--bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подсказка: как быстро убрать всю пунктуацию из текста и сразу переводить в нижний регистр\n",
        "\n",
        "import string\n",
        "\n",
        "test = \"Long, low-set dogs with sturdy bone, short legs, and a deep chest, Cardigans are powerful workers of deceptive speed and grace.\"\n",
        "\n",
        "test_clean = test.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
        "print(test_clean)\n",
        "\n",
        "# подробнее о методе .maketrans()\n",
        "# https://www.w3schools.com/python/ref_string_maketrans.asp "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnGp8fx8--y7",
        "outputId": "6a3162e2-d95c-44fe-f6ee-f9c862d17663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "long lowset dogs with sturdy bone short legs and a deep chest cardigans are powerful workers of deceptive speed and grace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ваше решение ниже:\n",
        "\n",
        "\n",
        "# в конце файла приведу разные решения с пары"
      ],
      "metadata": {
        "id": "PVPLIyML_3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Работа с NLTK\n",
        "\n",
        "### Давайте попробуем подготовить текст романа \"Преступление и наказание\" к анализу\n",
        "\n",
        "Проведем предобработку текста. Посмотрим на практике, на каком этапе нужна лемматизация.\n",
        "\n",
        "Краткий план:\n",
        "1. приведем все слова к нижнему регистру\n",
        "1. разобьем текст на слова (токенизация) с NLTK - nltk.tokenize\n",
        "1. удалим знаки препинания\n",
        "1. удалим стоп-слова\n",
        "\n",
        "На любом этапе можно считать часточность - Counter + стемминг"
      ],
      "metadata": {
        "id": "75EnFxyh9CNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# откроем файл в питоне\n",
        "with open('Dostoevsky_PrestuplenieINakazanie.txt', 'r', encoding='utf-8') as open_file: \n",
        "    text = open_file.read() # считаем файл в строку"
      ],
      "metadata": {
        "id": "Rt48CpY_7G3R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "tzuGUmxNB9w2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Убедимся, что в тексте лежит то что мы ожидаем:"
      ],
      "metadata": {
        "id": "bViid1pcCC2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# начало романа:\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HDYpUUICBDy",
        "outputId": "a8f4933a-590e-4ea5-caa4-7abcd19d6b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "преступление и наказание \n",
            "\n",
            "роман в шести частях с эпилогом\n",
            "\n",
            "  \n",
            "часть первая\n",
            "\n",
            "i\n",
            "\n",
            "   в начале июля, в \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# длина всего романа в символах\n",
        "print(len(text))"
      ],
      "metadata": {
        "id": "ZcHCSxxGCH1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b745f0-696a-417f-d652-05063bd361ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1094386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получим слова\n",
        "\n",
        "* Их частотности\n",
        "* Их сочетаемость друг с другом\n",
        "* Их распределение по тексту и т.д."
      ],
      "metadata": {
        "id": "VSSKHEv_Cu87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы знаем простой способ токенизировать строку — метод .split:"
      ],
      "metadata": {
        "id": "E3zx-YjOEpGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_string = 'Давайте еще раз   протестируем            токенизацию'\n",
        "some_string.split() "
      ],
      "metadata": {
        "id": "e11H8M9xC2bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e8ac60-2b62-4ba1-edbb-206a2297c53a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Давайте', 'еще', 'раз', 'протестируем', 'токенизацию']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь разделим \"Преступление и наказание\" и посчитаем в нем слова — хотя бы примерно:"
      ],
      "metadata": {
        "id": "Gts2Q-QcEyk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = text.split()\n",
        "print('Примерное количество слов в \"Преступлении и наказании\":', len(text_list))"
      ],
      "metadata": {
        "id": "F_do8eUlEula",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b22f34d-cae8-4b45-f05a-63e40ee1a257"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Примерное количество слов в \"Преступлении и наказании\": 176283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на верхушку этого списка\n",
        "print(text_list[:30])"
      ],
      "metadata": {
        "id": "5TRW36KRFBjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3e37d5-1aae-486a-f10a-0ddb70085c78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля,', 'в', 'чрезвычайно', 'жаркое', 'время,', 'под', 'вечер,', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки,', 'которую', 'нанимал']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = 'Аня пришла в Вышку, Никита пришел в Вышку'.split()\n",
        "print(tokens)\n",
        "print(tokens[3])\n",
        "print(tokens[7])\n",
        "tokens[3] == tokens[7]"
      ],
      "metadata": {
        "id": "TcUbSQgqFeMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ff61bb-1936-4ade-91ce-dc6b04c73703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Аня', 'пришла', 'в', 'Вышку,', 'Никита', 'пришел', 'в', 'Вышку']\n",
            "Вышку,\n",
            "Вышку\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на все слова, в которых есть подстрока 'топор'\n",
        "for token in text_list:\n",
        "  if 'топор' in token:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "jSV0ykmGFkud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc76b62b-04a7-49c0-bc13-a7bbeda3c211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "топором\n",
            "топор,\n",
            "топором...\n",
            "топора.\n",
            "топор\n",
            "топора,\n",
            "топорной\n",
            "топор.\n",
            "топором,\n",
            "топоре\n",
            "топор,\n",
            "топор,\n",
            "топора,\n",
            "топором\n",
            "топора!\n",
            "топор\n",
            "топор)\n",
            "топор,\n",
            "топор.\n",
            "топор\n",
            "топор...\n",
            "топор\n",
            "топор,\n",
            "топор\n",
            "топор\n",
            "топором,\n",
            "топор,\n",
            "топором\n",
            "топор,\n",
            "топор\n",
            "топором;\n",
            "топор\n",
            "топор.\n",
            "топор\n",
            "топор,\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор.\n",
            "топор?\n",
            "топоре.\n",
            "топора\n",
            "топор.\n",
            "топор\n",
            "топором\".\n",
            "топором\n",
            "топорами,\n",
            "топором,\n",
            "топором\n",
            "топор\n",
            "топор\n",
            "топора\n",
            "топором\n",
            "топором.\n",
            "топором.\n",
            "топор,\n",
            "топором,\n",
            "топором,\n",
            "топором,\n",
            "топором\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Более умный способ токенизации: делим текст регулярным выражением \n"
      ],
      "metadata": {
        "id": "JSihridMFwQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text_list_with_re = re.split(r\"[-@\\s.,)(\\\":;!?–\\n]+\", text)"
      ],
      "metadata": {
        "id": "ygQFU5L3F0N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_with_re[:30]"
      ],
      "metadata": {
        "id": "W5KBGxzkF_RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1cd9f8-3169-496c-8ac2-8df18c34b16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['преступление',\n",
              " 'и',\n",
              " 'наказание',\n",
              " 'роман',\n",
              " 'в',\n",
              " 'шести',\n",
              " 'частях',\n",
              " 'с',\n",
              " 'эпилогом',\n",
              " 'часть',\n",
              " 'первая',\n",
              " 'i',\n",
              " 'в',\n",
              " 'начале',\n",
              " 'июля',\n",
              " 'в',\n",
              " 'чрезвычайно',\n",
              " 'жаркое',\n",
              " 'время',\n",
              " 'под',\n",
              " 'вечер',\n",
              " 'один',\n",
              " 'молодой',\n",
              " 'человек',\n",
              " 'вышел',\n",
              " 'из',\n",
              " 'своей',\n",
              " 'каморки',\n",
              " 'которую',\n",
              " 'нанимал']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# проверим, что проблема приклеившейся пунктуации ушла\n",
        "for word in text_list_with_re:\n",
        "  if 'топор' in word:\n",
        "    print(word)"
      ],
      "metadata": {
        "id": "SczKlOqaGEa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913d8af7-c6c5-45f9-ca90-be4948a748a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "топором\n",
            "топор\n",
            "топором\n",
            "топора\n",
            "топор\n",
            "топора\n",
            "топорной\n",
            "топор\n",
            "топором\n",
            "топоре\n",
            "топор\n",
            "топор\n",
            "топора\n",
            "топором\n",
            "топора\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топором\n",
            "топор\n",
            "топором\n",
            "топор\n",
            "топор\n",
            "топором\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топор\n",
            "топоре\n",
            "топора\n",
            "топор\n",
            "топор\n",
            "топором\n",
            "топором\n",
            "топорами\n",
            "топором\n",
            "топором\n",
            "топор\n",
            "топор\n",
            "топора\n",
            "топором\n",
            "топором\n",
            "топором\n",
            "топор\n",
            "топором\n",
            "топором\n",
            "топором\n",
            "топором\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Еще более умный способ: сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK \n",
        "\n",
        "[Документация по NLTK](https://www.nltk.org/) и [книжка](https://www.nltk.org/book/)"
      ],
      "metadata": {
        "id": "HH5xf4oiGOHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "# в Colab уже есть, в других средах - запустите ячейку"
      ],
      "metadata": {
        "id": "gE1JTGwiGo9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb871617-37e6-46ea-ac49-f4784edbadde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
      ],
      "metadata": {
        "id": "RFPNuhlAGhPN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# это подгрузка вспомогательных данных, которые нужны nltk для токенизации\n",
        "from nltk import download\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "FYxGahd5UBcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dd1715-aeb2-4de3-8f4c-5dbb4f6c8afd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
        "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
        "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\"\"\"\n",
        "# текст отсюда - https://habr.com/ru/post/582620/"
      ],
      "metadata": {
        "id": "Zjy_KO28T1Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text_2)[:10])\n",
        "# wordpunct_tokenizer разбирает по регулярке - '\\w+|[^\\w\\s]+' \n",
        "# word_tokenize - тоже основан на регулярках, но более умных (учитывается последовательность некоторых символов, символы начала, конца слова и т.д).\n",
        "# для русского языка работает немного хуже, чем для английского"
      ],
      "metadata": {
        "id": "NPgNbDEgT177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae2c285-3a3e-42e3-d005-2ee8006487fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Задача', 'NLI', 'важна', 'для', 'компьютерных', 'лингвистов', ',', 'ибо', 'она', 'позволяет']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\"\n",
        "print(wordpunct_tokenize(sent))\n",
        "print(word_tokenize(sent))\n",
        "# I'm, it's, Sandy's, you'll, you're, don't и др.\n",
        "# пример отсюда: https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwDKHYHPgWjw",
        "outputId": "45279af7-b172-49db-8d0d-9c6733f8925e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'\", 'm', 'a', 'dog', 'and', 'it', \"'\", 's', 'great', '!', 'You', \"'\", 're', 'cool', 'and', 'Sandy', \"'\", 's', 'book', 'is', 'big', '.', 'Don', \"'\", 't', 'tell', 'her', ',', 'you', \"'\", 'll', 'regret', 'it', '!', \"'\", 'Hey', \"',\", 'she', \"'\", 'll', 'say', '!']\n",
            "['I', \"'m\", 'a', 'dog', 'and', 'it', \"'s\", 'great', '!', 'You', \"'re\", 'cool', 'and', 'Sandy', \"'s\", 'book', 'is', 'big', '.', 'Do', \"n't\", 'tell', 'her', ',', 'you', \"'ll\", 'regret', 'it', '!', \"'Hey\", \"'\", ',', 'she', \"'ll\", 'say', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# вернемся к Достоевскому, используем более умную лемматизацию\n",
        "text_list_nltk = word_tokenize(text)\n",
        "print(text_list_nltk[:100])\n",
        "\n",
        "len(text_list_nltk)"
      ],
      "metadata": {
        "id": "7M8E1sxJGwnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b0512a-988f-4bab-8be2-21ba5f541f5e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля', ',', 'в', 'чрезвычайно', 'жаркое', 'время', ',', 'под', 'вечер', ',', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки', ',', 'которую', 'нанимал', 'от', 'жильцов', 'в', 'с', '--', 'м', 'переулке', ',', 'на', 'улицу', 'и', 'медленно', ',', 'как', 'бы', 'в', 'нерешимости', ',', 'отправился', 'к', 'к', '--', 'ну', 'мосту', '.', 'он', 'благополучно', 'избегнул', 'встречи', 'с', 'своею', 'хозяйкой', 'на', 'лестнице', '.', 'каморка', 'его', 'приходилась', 'под', 'самою', 'кровлей', 'высокого', 'пятиэтажного', 'дома', 'и', 'походила', 'более', 'на', 'шкаф', ',', 'чем', 'на', 'квартиру', '.', 'квартирная', 'же', 'хозяйка', 'его', ',', 'у', 'которой', 'он', 'нанимал', 'эту', 'каморку', 'с']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221916"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Посчитаем частотность слов в нашем списке слов"
      ],
      "metadata": {
        "id": "H4nIAbE9HRao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы могли бы написать цикл, который считает упоминания каждого слова, перебирая в списке токенов слово за словом. А затем посчитать их при помощи словаря. \n",
        "\n",
        "Но! Мы воспользуемся встроенным в Python объектом Counter; он все это сделает сам."
      ],
      "metadata": {
        "id": "9vMjWIrgHcFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counter -- встроенный счетчик частотностей в Python"
      ],
      "metadata": {
        "id": "ZPmJIpx9HnhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "pY3X7cnOHXq9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter - это специальный объект, который умеет легко считать повторяющиеся элементы в iterable\n",
        "# Например в строке:\n",
        "char_freqs = Counter('aaaaabbbbcccddefghik') # получим частотности всех элементов строки\n",
        "print(char_freqs)\n",
        "print(char_freqs.most_common(3))"
      ],
      "metadata": {
        "id": "4pYvF8oMHqx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f8178b-2672-4164-c634-d111ca9bd051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1, 'f': 1, 'g': 1, 'h': 1, 'i': 1, 'k': 1})\n",
            "[('a', 5), ('b', 4), ('c', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = Counter(text_list_nltk) # отправим в Counter\n",
        "print(word_freqs.most_common(10)) # получим топ 10 слов"
      ],
      "metadata": {
        "id": "sUnjcX-cIAX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f101af6-da3b-48d3-f05e-e879406bb519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 25337), ('и', 8452), ('.', 8197), ('--', 6073), ('не', 3779), ('в', 3727), ('!', 3265), ('что', 3215), ('он', 2852), ('на', 2407)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Снова удалим пунктуацию"
      ],
      "metadata": {
        "id": "u7oNBTGlINjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# методом maketrans, посмотрим на string.punctuation\n",
        "print(string.punctuation)\n",
        "# чего не хватает?"
      ],
      "metadata": {
        "id": "c73a3bPASykt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a317acc-f225-4c71-836b-ec5a0850f727"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# зачем новый способ?\n",
        "\n",
        "print('john'.isalpha())\n",
        "print('1989'.isalpha())\n",
        "print(','.isalpha())\n",
        "print('диван-кровать'.isalpha()) # из-за дефиса тоже будет false"
      ],
      "metadata": {
        "id": "lvELGmdhIfFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc020ffe-7d76-4a65-e19e-ee413c63d330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_without_punkt = [] \n",
        "for word in text_list_nltk:\n",
        "    if word[0].isalpha(): # если в начале слова буква (а не знак препинания)\n",
        "        text_without_punkt.append(word)\n",
        "\n",
        "# на практике вы часто встретите строковые включения:\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
        "# краткая форма записи цикла for"
      ],
      "metadata": {
        "id": "ZkmeiGNAI1kL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_without_punkt[:30])"
      ],
      "metadata": {
        "id": "yQuKu10OJGz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77c2670-fbeb-46da-8526-ca491c2d9908"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля', 'в', 'чрезвычайно', 'жаркое', 'время', 'под', 'вечер', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки', 'которую', 'нанимал']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Посмотрим на частотность слов без пунктуации"
      ],
      "metadata": {
        "id": "FrbsmxrIJNsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_without_punkt).most_common(30))"
      ],
      "metadata": {
        "id": "0xhQQ7QwJRxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36121b91-da5d-41ea-9587-80a13ff9e8bd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('и', 8452), ('не', 3779), ('в', 3727), ('что', 3215), ('он', 2852), ('на', 2407), ('я', 2395), ('с', 2007), ('а', 1777), ('как', 1622), ('это', 1386), ('его', 1189), ('так', 1172), ('но', 1155), ('же', 1130), ('да', 1055), ('вы', 969), ('всё', 953), ('к', 919), ('она', 901), ('бы', 865), ('было', 786), ('еще', 701), ('у', 697), ('то', 688), ('даже', 680), ('по', 667), ('за', 650), ('ее', 625), ('только', 619)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Чистим от стоп-слов"
      ],
      "metadata": {
        "id": "O0jIlYlbJhMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно загрузить свои стоп-слова и удалить их, но проще взять из NLTK: там есть наборы стоп-слов для разных языков\n"
      ],
      "metadata": {
        "id": "D_Anp2_nJkVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "c6MXsMVgJg2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee418b4-6f9e-4e1f-bc25-7998637cae70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('russian') \n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "5GGXQfOOJtt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af10aa2-8737-4253-ace9-6b666dd7425b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean = []\n",
        "for word in text_without_punkt:\n",
        "  if word not in stop_words:\n",
        "    text_clean.append(word)\n",
        "\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
      ],
      "metadata": {
        "id": "M9EtTRs8J5Jl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_clean[:50])"
      ],
      "metadata": {
        "id": "eo7Ee7SEKSj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7354ac-a60e-4b46-d722-940c9a17cfb1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступление', 'наказание', 'роман', 'шести', 'частях', 'эпилогом', 'часть', 'первая', 'i', 'начале', 'июля', 'чрезвычайно', 'жаркое', 'время', 'вечер', 'молодой', 'человек', 'вышел', 'своей', 'каморки', 'которую', 'нанимал', 'жильцов', 'м', 'переулке', 'улицу', 'медленно', 'нерешимости', 'отправился', 'мосту', 'благополучно', 'избегнул', 'встречи', 'своею', 'хозяйкой', 'лестнице', 'каморка', 'приходилась', 'самою', 'кровлей', 'высокого', 'пятиэтажного', 'дома', 'походила', 'шкаф', 'квартиру', 'квартирная', 'хозяйка', 'которой', 'нанимал']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_clean).most_common(50))"
      ],
      "metadata": {
        "id": "ngpNxGtqKo_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256770c0-3c67-4fc0-8f46-7c08e6cc102b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('это', 1386), ('всё', 953), ('раскольников', 567), ('очень', 382), ('соня', 267), ('человек', 248), ('разумихин', 244), ('петрович', 208), ('время', 206), ('тебе', 204), ('дело', 200), ('тотчас', 186), ('минуту', 177), ('ивановна', 175), ('сказал', 166), ('мог', 164), ('стало', 161), ('кажется', 157), ('что-то', 156), ('сказать', 155), ('стал', 151), ('знаю', 150), ('точно', 149), ('свидригайлов', 142), ('как-то', 141), ('дуня', 138), ('порфирий', 132), ('несколько', 131), ('прямо', 129), ('раскольникова', 128), ('катерина', 127), ('действительно', 124), ('всем', 122), ('совершенно', 121), ('петр', 120), ('вчера', 118), ('особенно', 117), ('руки', 116), ('весьма', 116), ('хотя', 114), ('именно', 111), ('пульхерия', 110), ('александровна', 109), ('слишком', 108), ('глаза', 108), ('хотел', 107), ('дверь', 107), ('весь', 106), ('пошел', 105), ('лицо', 103)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Стемминг в Python \n",
        "\n",
        "Самый простой способ автоматический нормализации слов в языках с морфологией — стемминг. Стемминг — это очень грубое разбиение формы на предполагаемую основу и предполагаемую флексию. \n",
        "\n",
        "Программы-стеммеры умеют превращать:\n",
        "* \"Vyshka's students coded\" в \"Vyshka student code\"\n",
        "* 'Маша поехала за грибами' в 'Маш поехал за гриб'\n",
        "* 'Даня работает в Вышке' в \"Дан работа в Вышк\"\n",
        "\n",
        "Как можно догадаться из этих примеров, стемминг — не лучшее (и крайне непопулярное) решение для языков типа русского. Он лучше подходит для английского. \n",
        "\n",
        "В NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее"
      ],
      "metadata": {
        "id": "Jzd9pOcYK4QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK стемминг"
      ],
      "metadata": {
        "id": "KD6lyVLfLJDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
        "\n",
        "Подробнее про стеммер Портера можно почитать [вот тут](https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340)\n",
        "\n",
        "А совсем подробнее [вот тут](http://snowball.tartarus.org/algorithms/russian/stemmer.html)"
      ],
      "metadata": {
        "id": "eC3HE7FsVOca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer \n",
        "stemmer = SnowballStemmer(\"russian\") # в эту переменную мы сохраним уже готовый объект-стеммер для русского"
      ],
      "metadata": {
        "id": "OrZ65rHiLGu3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## предположите, что выдаст?\n",
        "print(stemmer.stem('университетами'))\n",
        "print(stemmer.stem('мышам'))\n",
        "print(stemmer.stem('конями'))\n",
        "print(stemmer.stem('людей'))"
      ],
      "metadata": {
        "id": "deFj6YY_LRZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59df4b5-0b2b-4f92-cce3-4302a41fb302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "университет\n",
            "мыш\n",
            "кон\n",
            "люд\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## стеммер не умеет сам токенизировать -- он работает только с отдельными словами. \n",
        "## поэтому надо идти по словам циклом\n",
        "text_stemmed = []\n",
        "for word in text_clean[:1000]:\n",
        "  text_stemmed.append(stemmer.stem(word))\n",
        "\n",
        "# аналогично text_stemmed = [stemmer.stem(word) for word in text_clean[:1000]]\n",
        "print(text_stemmed[:20])"
      ],
      "metadata": {
        "id": "k8qc06_DLTIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51497b65-7770-4f1e-a513-ddae7a197c08"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступлен', 'наказан', 'рома', 'шест', 'част', 'эпилог', 'част', 'перв', 'i', 'начал', 'июл', 'чрезвычайн', 'жарк', 'врем', 'вечер', 'молод', 'человек', 'вышел', 'сво', 'каморк']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_stemmed).most_common(50))"
      ],
      "metadata": {
        "id": "xr30gE4_P23o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a357cb-5e0a-43e7-d86a-efcd02d90418"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('эт', 16), ('человек', 13), ('молод', 12), ('котор', 12), ('все', 12), ('квартир', 10), ('сво', 9), ('улиц', 9), ('так', 9), ('лестниц', 8), ('врем', 7), ('очен', 7), ('вся', 7), ('встреч', 6), ('дом', 6), ('комнат', 6), ('хозяйк', 5), ('сам', 5), ('одн', 5), ('дел', 5), ('подума', 5), ('болта', 5), ('особен', 5), ('кажд', 4), ('выход', 4), ('всяк', 4), ('бо', 4), ('стоя', 4), ('несмотр', 4), ('знал', 4), ('мелоч', 4), ('стар', 4), ('мебел', 4), ('крошечн', 4), ('част', 3), ('каморк', 3), ('проход', 3), ('мим', 3), ('какое-т', 3), ('боя', 3), ('похож', 3), ('рук', 3), ('нос', 3), ('шаг', 3), ('слишк', 3), ('дела', 3), ('месяц', 3), ('угл', 3), ('чувств', 3), ('мелькнул', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Краткий алгоритм для любого текста с NLTK"
      ],
      "metadata": {
        "id": "H4bmxuREioDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# все то, что надо импортировать из NLTK\n",
        "#!pip install nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian') \n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "stemmer = SnowballStemmer(\"russian\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ0HzYXqjfVD",
        "outputId": "8d5148e8-6d23-49aa-a4ec-d56d644e3674"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower() # нижний регистр"
      ],
      "metadata": {
        "id": "YUX0Ngxoi13V"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_nltk = word_tokenize(text) # токенизация\n",
        "print(text_list_nltk[:100])\n",
        "print(len(text_list_nltk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl8GvAcdjUG9",
        "outputId": "1107b403-f461-4719-cf78-f3d656ff3917"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля', ',', 'в', 'чрезвычайно', 'жаркое', 'время', ',', 'под', 'вечер', ',', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки', ',', 'которую', 'нанимал', 'от', 'жильцов', 'в', 'с', '--', 'м', 'переулке', ',', 'на', 'улицу', 'и', 'медленно', ',', 'как', 'бы', 'в', 'нерешимости', ',', 'отправился', 'к', 'к', '--', 'ну', 'мосту', '.', 'он', 'благополучно', 'избегнул', 'встречи', 'с', 'своею', 'хозяйкой', 'на', 'лестнице', '.', 'каморка', 'его', 'приходилась', 'под', 'самою', 'кровлей', 'высокого', 'пятиэтажного', 'дома', 'и', 'походила', 'более', 'на', 'шкаф', ',', 'чем', 'на', 'квартиру', '.', 'квартирная', 'же', 'хозяйка', 'его', ',', 'у', 'которой', 'он', 'нанимал', 'эту', 'каморку', 'с']\n",
            "221916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# удалить пунктуацию из списка токенов\n",
        "\n",
        "text_without_punkt = [] \n",
        "for word in text_list_nltk:\n",
        "    if word[0].isalpha(): # если в начале слова буква (а не знак препинания)\n",
        "        text_without_punkt.append(word)\n",
        "\n",
        "# краткая запись:\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
      ],
      "metadata": {
        "id": "kyN0lpYdjT3s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# чистим от стоп-слов\n",
        "\n",
        "text_clean = []\n",
        "for word in text_without_punkt:\n",
        "  if word not in stop_words:\n",
        "    text_clean.append(word)\n",
        "\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
      ],
      "metadata": {
        "id": "mCia_98vjYA-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# стемминг\n",
        "\n",
        "text_stemmed = []\n",
        "for word in text_clean[:1000]:\n",
        "  text_stemmed.append(stemmer.stem(word))\n",
        "\n",
        "# аналогично text_stemmed = [stemmer.stem(word) for word in text_clean[:1000]]\n",
        "print(text_stemmed[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6bvXrpVkpHY",
        "outputId": "772db436-b307-4e8b-86c7-b2580c6b0d92"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['преступлен', 'наказан', 'рома', 'шест', 'част', 'эпилог', 'част', 'перв', 'i', 'начал', 'июл', 'чрезвычайн', 'жарк', 'врем', 'вечер', 'молод', 'человек', 'вышел', 'сво', 'каморк']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# стемминга недостаточно - позже добавим лемматизацию и более умный подсчет частот"
      ],
      "metadata": {
        "id": "CcYOFlhxk4Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Здесь разные решения задачи по отзывам на фильм"
      ],
      "metadata": {
        "id": "WGADmwFneCgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paul_n = paul.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()\n",
        "paul_n = set(paul_n)\n",
        "print(\"paul\", len(paul_n))\n",
        "\n",
        "jane_n = jane.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()\n",
        "jane_n = set(jane_n)\n",
        "print(\"jane\", len(jane_n))\n",
        "\n",
        "kate_n = kate.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()\n",
        "kate_n = set(kate_n)\n",
        "print(\"kate\", len(kate_n))\n",
        "\n",
        "nick_n = nick.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()\n",
        "nick_n = set(nick_n)\n",
        "print(\"nick\", len(nick_n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfswf11OkCNL",
        "outputId": "27d03b83-948f-4188-f6bd-810c1afea0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paul 168\n",
            "jane 129\n",
            "kate 208\n",
            "nick 174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Предобработать строку, сведя все к нижнему регистру, убрать пунктуацию.\n",
        "\n",
        "paul_lc=paul.translate(str.maketrans(\"\",\"\",string.punctuation)).lower()\n",
        "print(paul_lc)\n",
        "jane_lc=jane.translate(str.maketrans(\"\",\"\",string.punctuation)).lower()\n",
        "kate_lc=kate.translate(str.maketrans(\"\",\"\",string.punctuation)).lower()\n",
        "nick_lc=nick.translate(str.maketrans(\"\",\"\",string.punctuation)).lower()\n",
        "\n",
        "#2. Превратить строку в список \n",
        "dir(list) #check methods\n",
        "paul_lc_lst=list(paul_lc.split())\n",
        "paul_lc_lst_st=set(list(paul_lc.split()))\n",
        "\n",
        "len(paul_lc_lst)\n",
        "len(paul_lc_lst_st)\n",
        "jane_lc_lst= list(jane_lc.split())\n",
        "kate_lc_lst=  list(kate_lc.split())\n",
        "nick_lc_lst=  list(nick_lc.split())\n",
        "#check data\n",
        "import pandas as pd\n",
        "paul_lc_lst_df=  pd.DataFrame(paul_lc_lst)\n",
        "#\n",
        "paul_lc_lst_df.describe()\n",
        "\n",
        "#road to DF\n",
        "paul_lc_lst_df.value_counts()\n",
        "paul_dict=dict(paul_lc_lst_df.value_counts())\n",
        "#failed\n",
        "#3. Оставить уникальные элементы в списке (превратить список во множество)\n",
        "#4. Определить размер такого множества\n",
        "paul_lc_lst_df=  pd.DataFrame(paul_lc_lst)\n",
        "paul_lc_lst_df.describe()\n",
        "paul_lc_lst_df=  pd.DataFrame(paul_lc_lst)\n",
        "\n",
        "jane_lc_lst_df=  pd.DataFrame(jane_lc_lst)\n",
        "kate_lc_lst_df=  pd.DataFrame(kate_lc_lst)\n",
        "nick_lc_lst_df=  pd.DataFrame(nick_lc_lst)\n",
        "print(paul_lc_lst_df.describe())\n",
        "print(jane_lc_lst_df.describe())\n",
        "print(kate_lc_lst_df.describe())\n",
        "print(nick_lc_lst_df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OVWWNYKkRcq",
        "outputId": "513b5532-9b59-4fe1-b26e-18a7999a0241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when this film first came out in 1980 i remember going to see it on opening night this movie just scared the life out of me which is what still happens every time\n",
            "i rent the video for a rewatchi have seen the shining at least six or seven times and i still believe it to be simultaneously and paradoxically one of the most frightening and yet funniest films ive ever seen frightening because of the extraordinarily effective use of long shots to create feelings of isolation convex lens shots to enhance surrealism and meticulously scored music to bring tension levels to virtually unbearable levels and funny because of jack nicholsons outrageous and in many cases adlibbed onscreen antics it never ceases to amaze me how the shining is actually two films in one both a comedy and a horror flick ghostly apparitions of a strikingly menacing nature haunt much of the first half of the film which gradually evolve into ever more serious physical threats as time progresses be that as it may there is surprisingly little violence given the apparent intensity but that is little comfort \n",
            "for the feint of heart as much of the terror is more implied than manifest the shining is a truly frightening movie that works symbolically on many levels but is basically  about human shortcomings and the way they can be exploited by unconscious forces combined with weakness of will this film scares the most just by using suggestion to turn your own imagination against you the shining is a brilliant cinematic masterpiece the likes of which have never been seen before or since highly highly recommended\n",
            "          0\n",
            "count   274\n",
            "unique  168\n",
            "top     the\n",
            "freq     16\n",
            "          0\n",
            "count   211\n",
            "unique  129\n",
            "top     the\n",
            "freq     21\n",
            "          0\n",
            "count   301\n",
            "unique  208\n",
            "top     the\n",
            "freq     18\n",
            "          0\n",
            "count   285\n",
            "unique  174\n",
            "top     the\n",
            "freq     20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import math\n",
        "\n",
        "def normalize(text):\n",
        "    normalized = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
        "    return normalized\n",
        "\n",
        "\n",
        "clear_paul = len(set(normalize(paul)))\n",
        "clear_jane = len(set(normalize(jane)))\n",
        "clear_kate = len(set(normalize(kate)))\n",
        "clear_nick = len(set(normalize(nick)))\n",
        "print(max(clear_paul, clear_jane, clear_kate, clear_nick))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DexO1vO8ngk5",
        "outputId": "98731f24-db85-4ed2-d4b1-739737d8bc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = [paul, jane, kate, nick]\n",
        "for i in names:\n",
        "  a = i.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
        "  print(len(set(a)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV37wUp_n5nv",
        "outputId": "035dba2c-82fe-469f-9091-6c9853dcc1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168\n",
            "129\n",
            "208\n",
            "174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def normalize(name):\n",
        "    normalized = name.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
        "    return normalized\n",
        "\n",
        "names_dict = {'Paul': len(set(normalize(paul))), 'Jane': len(set(normalize(jane))), 'Kate': len(set(normalize(kate))), 'Nick': len(set(normalize(nick)))}\n",
        "\n",
        "#print(names_dict)\n",
        "\n",
        "maxcount = None\n",
        "maxname = None\n",
        "for name,count in names_dict.items():\n",
        "    if maxcount is None or count > maxcount:\n",
        "        maxname = name\n",
        "        maxcount = count\n",
        "        \n",
        "print('Самый богатый лексикон у', maxname, '-', maxcount, 'уникальныых слов.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xwQNf5ddWIb",
        "outputId": "dc1a6e5e-292d-4701-b9eb-acdda045eaa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Самый богатый лексикон у Kate - 208 уникальныых слов.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxcount = None\n",
        "print(type(maxcount))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd7TvBd8etT4",
        "outputId": "20a42fd6-2f18-463e-eb1e-af31e487f0e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Доп. задание\n",
        "\n"
      ],
      "metadata": {
        "id": "V8HSb8QdPwZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "давайте поизвлекаем текст из датасетов и поисследуем его, например, составив частотный список (с нормализацией, удалением пунктуации, токенизацией и удалением стоп-слов)\n",
        "\n",
        "Датасет [здесь](https://raw.githubusercontent.com/AnnSenina/Python_for_CL/main/data/elonmusk.csv)"
      ],
      "metadata": {
        "id": "-hx0FkoNP_MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('elonmusk.csv', encoding=\"utf-8\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mqIfAEZNQKfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tweet\"] # в этой колонке нужные нам данные"
      ],
      "metadata": {
        "id": "Getf_gdxRZLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# как достать текст из колонок датафрейма\n",
        "tweets = df[\"tweet\"].to_list()\n",
        "print(tweets)"
      ],
      "metadata": {
        "id": "FVG3ZZwwRc1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# дальше работаем с переменной tweets\n",
        "# ваш код ниже\n",
        "\n"
      ],
      "metadata": {
        "id": "rsq1lJH5RjxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}