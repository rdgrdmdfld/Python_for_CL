{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_for_CL/blob/main/notebooks/Python_10_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Скрейперы, краулеры, парсеры\n",
        "\n",
        "Веб-скрейпинг = краулинг + парсинг (приблизительно!)\n",
        "\n",
        "Сегодня веб-скрейпинг и парсинг используются как синонимы"
      ],
      "metadata": {
        "id": "tQiBPzaI6F4y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2LDC-PbXhtR"
      },
      "source": [
        "В этой тетрадке мы поговорим о способах собрать свой датасет для исследований: откуда брать данные, как их собирать и как хранить."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHJEqIWOqGoj"
      },
      "source": [
        "Существует несколько библиотек(модулей) для работы с веб-страничками, сегодня мы будем использовать requests для доступа к веб-страничкам и Beautiful Soup для работы с содержимым html-документов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T07:43:42.215240Z",
          "start_time": "2020-12-19T07:43:42.162193Z"
        },
        "id": "ncJ-uAqjDNkC",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install requests #ставим модуль requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T07:11:35.137512Z",
          "start_time": "2020-12-19T07:11:33.589516Z"
        },
        "id": "fFcBQ25GZBCO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ставим модуль beautifulsoup, самая последняя версия - четвертая\n",
        "!pip install beautifulsoup4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:18:38.198003Z",
          "start_time": "2020-12-19T08:18:38.186002Z"
        },
        "id": "XT5G41FYnr7a"
      },
      "outputs": [],
      "source": [
        "# импортируем модули в тетрадку\n",
        "\n",
        "import requests as rq\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvQNBqUYwE8"
      },
      "source": [
        "# Как работать с веб-страничками"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Oy0NnLnk0L"
      },
      "source": [
        "### Шаг 1. \n",
        "\n",
        "Создадим переменную ```url``` и сохраним в нее адрес какой-нибудь html-страницы: например, учебной страницы, созданной в Вышке\n",
        "\n",
        "обратите внимание, что адрес прописываем в кавычках, как строку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:22:23.420826Z",
          "start_time": "2020-12-19T08:22:23.416822Z"
        },
        "id": "WzqN6Ss_aujY"
      },
      "outputs": [],
      "source": [
        "url = 'https://online.hse.ru/python-as-foreign/1/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysgl6VmpKjxu"
      },
      "source": [
        "В модуле requests есть метод request.get(), который сохраняет ответ сервера на наш реквест. Мы применим его к переменной url, куда сохранен путь к странице. \n",
        "Сохраним результат в переменную page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:46:54.806718Z",
          "start_time": "2020-12-19T08:46:54.506720Z"
        },
        "id": "O8AH4kDxqKvF"
      },
      "outputs": [],
      "source": [
        "page = rq.get(url) \n",
        "\n",
        "print(page) # посмотрим на код ответа, если 200, все хорошо\n",
        "print(type(page))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Y2C39Gp0yK"
      },
      "source": [
        "код 200 сообщает, что страница загружена успешно \n",
        "*(коды, начинающиеся с 2, обычно указывают на успешное выполнение операции, а коды, начинающиеся с 4 или 5, сообщают об ошибке)*\n",
        "\n",
        "Узнать больше о кодах состояния HTTP  можно [по этой ссылке.](https://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-01#Status-Codes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# есть еще одна команда, чтобы получить код в виде числа:\n",
        "print(page.status_code)\n",
        "print(type(page.status_code))"
      ],
      "metadata": {
        "id": "OgWJ0rs36tsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gg53fJKNObR"
      },
      "source": [
        "Следующим шагом нужно получить доступ к текстовому содержимому веб-файлов.\n",
        "\n",
        "Здесь нам поможет page.text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:47:01.231718Z",
          "start_time": "2020-12-19T08:47:01.108719Z"
        },
        "id": "md61SNPYu_jL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(page.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page.encoding = 'utf-8' # укажем кодировку и снова посмотрим\n",
        "print(page.text)"
      ],
      "metadata": {
        "id": "zXIpm0FkVY3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cEfTSEYhcY"
      },
      "source": [
        "### Шаг2\n",
        "\n",
        "Поработаем с текстом на страничке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcVMhwpHNxE5"
      },
      "source": [
        "Мы получили текст страницы (со всеми html-тегами), однако его неудобно прочитать в таком виде. \n",
        "\n",
        "Здесь нам понадобится Beautiful Soup, модуль для html-парсинга: он сделает текст веб-страницы, извлеченный с помощью Requests, более читаемым, потому что создает дерево синтаксического разбора из проанализированных тэгов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T09:20:00.809844Z",
          "start_time": "2020-12-19T09:20:00.351849Z"
        },
        "id": "GOBPlBro3AQR"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(page.text, features=\"html.parser\") #сохраним результат в переменную soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:47:09.815788Z",
          "start_time": "2020-12-19T08:47:09.553787Z"
        },
        "id": "HigX0lNr3P1Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(soup.prettify()) # показывает нашу страницу в красивом виде"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjED1MGgqGot"
      },
      "source": [
        "### Шаг3 \n",
        "время доставать тэги - но сначала...\n",
        "\n",
        "### Немного про html\n",
        "\n",
        "Перейдем на сайт [w3schools.com](https://www.w3schools.com/Html/) и откроем раздел Try it yourself.\n",
        "\n",
        "Задание 1: \n",
        "\n",
        "1.   внутри тэгов h1 напишите ваши Ф.И.О.\n",
        "\n",
        "2.   напишите подзаголовок \"О себе\", добавив тэги h2\n",
        "\n",
        "3.   внутри тэгов p напишите \"Я учусь создавать html-страницы.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2\n",
        "\n",
        "Добавим таблицу\n",
        "\n",
        "<table border=\"1\">\n",
        "<tr>\n",
        "<th>Фамилия</th>\n",
        "<th>Имя</th>\n",
        "<th>Возраст</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td></td>\n",
        "<td></td>\n",
        "<td></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "1. Заполните строку таблицы\n",
        "\n",
        "2. Добавьте еще один столбец и заполните его"
      ],
      "metadata": {
        "id": "MqiVxN6jEphg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Бонус! pandas и сам умеет собирать таблицы с сайтов, без rq, BeautifulSoup"
      ],
      "metadata": {
        "id": "x3x43B4Qypk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "banks = pd.read_html('https://cbr.ru/currency_base/daily/')\n",
        "print(banks) # возвращается список датафреймов - потому что вдруг таблиц на сайте несколько"
      ],
      "metadata": {
        "id": "ANg-jm6OxiOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(banks[0]) # 0 объект списка - наша едиснтвенная табличка на странице\n",
        "data.to_csv(\"banks.csv\")"
      ],
      "metadata": {
        "id": "xgLK7BdkxxZh"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "banks.to_csv(\"banks.csv\")"
      ],
      "metadata": {
        "id": "liXEgQ7QyINl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVR8OVuRUaO1"
      },
      "source": [
        "###Вернемся к тегам:\n",
        "\n",
        "предыдущие шаги со страницей позволили привести веб-страничку к виду, где содержание каждого тега написано с новой строки. \n",
        "\n",
        "Некоторые теги полезны для конкретной задачи (там текст), некоторые - не очень (например, мета-данные, картинки и тд)\n",
        "\n",
        "Извлечь одинаковые теги со страницы можно с помощью метода find_all(). Он похож на метод регулярок, с которым мы работали: он вернет все экземпляры данного тега в документе. Нужно прописать в скобках метода нужный тег. \n",
        "\n",
        "### Самые популярные теги:\n",
        "    <h1-h6> - заголовки\n",
        "    <div> для целых \"блоков\" странички\n",
        "    <li> список с перечислением\n",
        "    <p> для текста\n",
        "    <a> для гиперссылок\n",
        "    <img> для изображений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9jO4I8JsqGou"
      },
      "outputs": [],
      "source": [
        "soup.find_all(\"head\") # тег нужно записать без треугольных скобочек\n",
        "#print(soup.find_all(\"head\")) # для PyCharm\n",
        "\n",
        "# попробуйте теги body, title, a и др."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ZsmEvLqGov"
      },
      "source": [
        "А так можно достать нужные части тега (напимер, текст)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:50:08.027471Z",
          "start_time": "2020-12-19T08:50:08.007477Z"
        },
        "id": "N0RXeVL33W7I",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for x in soup.find_all('a'):\n",
        "    print(x.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JfaHd0lqGov"
      },
      "outputs": [],
      "source": [
        "# Весь текст на страничке за раз можно достать еще и так\n",
        "print(soup.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPczVAd5qGov"
      },
      "source": [
        "## Как создать корпус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euAwh4wPqGow"
      },
      "source": [
        "Итак, мы определили нужные теги и напарсили необходимые данные. Пора сохранить их в файл. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuqOyfJwqGow"
      },
      "outputs": [],
      "source": [
        "# в .txt\n",
        "\n",
        "with open(\"HP.txt\", \"w\") as file:\n",
        "    for x in soup.find_all(\"a\"):\n",
        "        file.write(x.text)\n",
        "        \n",
        "# необязательно, но можем скорретировать информацию, которую записываем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdUmUfmaqGow"
      },
      "outputs": [],
      "source": [
        "#   или в .csv\n",
        "\n",
        "names = []\n",
        "for x in soup.find_all(\"a\"):\n",
        "  names.append(x.text)\n",
        "\n",
        "links = []\n",
        "for link in soup.find_all(\"a\"):\n",
        "  links.append(link.get('href')) # вот так можно достать ссылку из тега a\n",
        "\n",
        "print(names)\n",
        "print(links)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(len(names)): # списки равной длины, переберем поиндексно\n",
        "  l = []\n",
        "  l.append(names[i])\n",
        "  l.append('https://online.hse.ru/python-as-foreign/1/' + links[i])\n",
        "  data.append(l)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "_MsSRmwgY4Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"name\", \"url\"])\n",
        "\n",
        "df\n",
        "# print(df) #для PyCharm"
      ],
      "metadata": {
        "id": "G4ZdcBlUZU06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqYjNU9dqGox"
      },
      "outputs": [],
      "source": [
        "# сохраняем\n",
        "df.to_csv(\"HP.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdOaCoseqGoy"
      },
      "source": [
        "## Практика\n",
        "\n",
        "давайте попарсим другой адрес и вытащим оттуда весь текст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRw_mCmXqGoy"
      },
      "outputs": [],
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Welsh_Corgi\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код ниже\n",
        "# передаем в rq\n",
        "\n"
      ],
      "metadata": {
        "id": "csD84n82cBt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# передаем в BeautifulSoup\n"
      ],
      "metadata": {
        "id": "QDv9Tgz-cF2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сохраняем\n"
      ],
      "metadata": {
        "id": "xGlsPkkTcIIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Бонус (в т.ч. для домашнего задания)\n",
        "\n",
        "Как заставить код спарсить несколько страничек"
      ],
      "metadata": {
        "id": "kwtSDxP_cLOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://nplus1.ru/' # сохраняем\n",
        "page = rq.get(url) # загружаем страницу по ссылке\n",
        "print(page.status_code)  # 200 - страница загружена"
      ],
      "metadata": {
        "id": "UMiTiwiDcTs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "E4Bf__jRceK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 1 - соберем с главной страницы ссылки на те, которые хотим спарсить"
      ],
      "metadata": {
        "id": "3QwH7zace4sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = []\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "    urls.append(link.get('href'))\n",
        "\n",
        "urls\n",
        "#print(urls)\n",
        "\n",
        "# много лишних ссылок и повторов, оставим только ссылки на новости"
      ],
      "metadata": {
        "id": "08GU8o_pcnG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_urls = []\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "  if 'news' in link.get('href') and 'https://nplus1.ru' in link.get('href') and link.get('href') not in full_urls:\n",
        "    full_urls.append(link.get('href')) \n",
        "\n",
        "full_urls\n",
        "# print(full_urls)"
      ],
      "metadata": {
        "id": "j5_a2Fz2c7zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# первая ссылка - попала в выдачу не совсем корректно (тематически нам не подходит), ее можно удалить или позже указать срез ссылок для обработки, пропустив ее\n",
        "# удалим сразу\n",
        "\n",
        "del full_urls[0]\n",
        "print(full_urls)"
      ],
      "metadata": {
        "id": "kt_BuUBjg36M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 2 - посмотрим, что можно спарсить со страницы 1 любой новости"
      ],
      "metadata": {
        "id": "Zyi6cWrPe6MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url0 = full_urls[0]\n",
        "\n",
        "page0 = rq.get(url0)\n",
        "soup0 = BeautifulSoup(page0.text)\n",
        "print(soup0.prettify())"
      ],
      "metadata": {
        "id": "KroD1763c-I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# случай сложный: самая интересная информация хранится в теге meta\n",
        "\n",
        "soup0.find_all('meta')\n",
        "#print(soup0.find_all('meta'))"
      ],
      "metadata": {
        "id": "Bxff_Dfofdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберем информацию об авторе, дате публикации, заголовке \n",
        "Какие атрибуты нам нужны? \n",
        "\n",
        "author, datePublished, og:title"
      ],
      "metadata": {
        "id": "Egn2nWLnfJZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# атрибуты прописаны внутри тега meta, как их извлечь?\n",
        "# передадим нужный атрибут как словарь \n",
        "\n",
        "author = soup0.find_all('meta', {'name' : 'author'})[0].attrs['content'] # вызовем автора по ключу (content) и сохраним в переменную\n",
        "date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].attrs['content']\n",
        "title = soup0.find_all('meta', {'property' : 'og:title'})[0].attrs['content']\n",
        "\n",
        "print(author, date, title)"
      ],
      "metadata": {
        "id": "PnEXVZccfKYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечем текст новости, используя тэг p с атрибутом mb-6"
      ],
      "metadata": {
        "id": "hJywp95fhURo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = soup0.find_all('p', {'class' : 'mb-6'})\n",
        "text_list\n",
        "#print(text_list)"
      ],
      "metadata": {
        "id": "cT6eJ8SUhRu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for i in text_list:\n",
        "  text.append(i.text)\n",
        "text\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "oiMw6r17iTbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# слегка поправим текст:\n",
        "final_text = ' '.join(text)\n",
        "final_text = final_text.replace('\\xa0', ' ')\n",
        "final_text\n",
        "#print(final_text)"
      ],
      "metadata": {
        "id": "gg_9SEt6immG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 3: напишем функцию, которая будет идти по ссылкам из full_urls и парсить каждую страницу"
      ],
      "metadata": {
        "id": "Hi6THR89i5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetNews(url0):\n",
        "  page0 = rq.get(url0)\n",
        "  soup0 = BeautifulSoup(page0.text, features=\"html.parser\")\n",
        "  author = soup0.find_all('meta', {'name' : 'author'})[0].attrs['content'] \n",
        "  date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].attrs['content']\n",
        "  title = soup0.find_all('meta', {'property' : 'og:title'})[0].attrs['content']\n",
        "  text_list = soup0.find_all('p', {'class' : 'mb-6'})\n",
        "  text = []\n",
        "  for i in text_list:\n",
        "    text.append(i.text)\n",
        "  final_text = ' '.join(text)\n",
        "  final_text = final_text.replace('\\xa0', ' ')\n",
        "  return url0, author, date, title, final_text"
      ],
      "metadata": {
        "id": "IY59cx7ei4IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = [] # список с новостями\n",
        "\n",
        "for link in full_urls:\n",
        "  new = GetNews(link)\n",
        "  news.append(new)"
      ],
      "metadata": {
        "id": "IhoOb2fYjKSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(news)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l3nBbYTWjVvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['link', 'author', 'date', 'title', 'text']"
      ],
      "metadata": {
        "id": "B-nDj4NyjY13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "LcQpUHS2jntE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('nplus_news.xlsx')"
      ],
      "metadata": {
        "id": "6GeygSd9jxl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как парсить сайты, не раздражая сервера"
      ],
      "metadata": {
        "id": "QQpr82aNkOOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выше мы пробовали скрейпить сайт циклом. Скорость такого скрейпинга ограничена только скоростью сети и быстродействием компьютеров. То есть может быть довольно высокой\n",
        "\n",
        "Это нагрузка на сервера, и иногда владельцы блокируют скрейперов"
      ],
      "metadata": {
        "id": "6QfkgcxbkRk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "for i in range(10): # 10 раз запроси одну и ту же страницу\n",
        "    rq.get('https://quotes.toscrape.com')"
      ],
      "metadata": {
        "id": "H8kwG5ADkgy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Использовать sleep из модуля time"
      ],
      "metadata": {
        "id": "Rwp77g5Ukcg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "St6FpgbJlAt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "for i in range(10):\n",
        "    rq.get('https://quotes.toscrape.com')\n",
        "    time.sleep(1) # ждем 1 секунду после каждого шага"
      ],
      "metadata": {
        "id": "_2R3les6kPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Мимикрировать под человека с помощью fake_useragent"
      ],
      "metadata": {
        "id": "O9xoHliKlbld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Допустим, вы решили парсить мемы:"
      ],
      "metadata": {
        "id": "CKp3fL4UlhsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_link = 'https://knowyourmeme.com/memes/all/page/1'\n",
        "response = rq.get(page_link)"
      ],
      "metadata": {
        "id": "xyDGQxailfVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.text)\n",
        "print(soup.find(\"table\")) # спасим таблицу с мемами"
      ],
      "metadata": {
        "id": "gzqMDVg_ln_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.status_code)"
      ],
      "metadata": {
        "id": "ns_O7AeMl0as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)\n",
        "# для нас запрещено!\n",
        "# сервер видит, что к нему пришла программа, а не человек"
      ],
      "metadata": {
        "id": "E-3AcB16l1fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fake_useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49SuOg4slySL",
        "outputId": "511de7a8-8fc3-441e-9c4b-2095902a052f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.1.0-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata~=4.0 in /usr/local/lib/python3.7/dist-packages (from fake_useragent) (4.13.0)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.7/dist-packages (from fake_useragent) (5.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.0->fake_useragent) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.0->fake_useragent) (3.10.0)\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fake_useragent import UserAgent\n",
        "print(UserAgent().chrome)"
      ],
      "metadata": {
        "id": "lbdRMx4KmDJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_link = 'https://knowyourmeme.com/memes/all/page/1'\n",
        "response = rq.get(page_link, headers={'User-Agent': UserAgent().chrome})"
      ],
      "metadata": {
        "id": "ufdxvq4umHUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.status_code)"
      ],
      "metadata": {
        "id": "ywW_RAhOmPSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "id": "T_BbU055mRBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.text)\n",
        "soup.find('table')"
      ],
      "metadata": {
        "id": "ahE_m4clmNGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. В крайних случаях можно использовать тор/прокси-серверы для смены IP\n",
        "\n",
        "* [Раз](https://habr.com/ru/company/ods/blog/346632/#22-tor---syn-odina)\n",
        "* [Два](https://stackoverflow.com/questions/30286293/make-requests-using-python-over-tor)\n"
      ],
      "metadata": {
        "id": "S19Su_LdoTOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Полезный совет про try / except!\n",
        "\n",
        "Когда парсите много страниц разом, велик риск получить ошибки (какая-то страница не ответила, сервер начал нас блокировать и т.д.)\n",
        "\n",
        "Используйте try / except"
      ],
      "metadata": {
        "id": "rWlWI_6aoGPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "    print(100/i)\n",
        "\n",
        "print('Все в порядке!')"
      ],
      "metadata": {
        "id": "BKRWxU8uoFgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "    try:\n",
        "        print(100/i)\n",
        "    except: \n",
        "        print(f'с числом {i} не работает')\n",
        "\n",
        "print('Все в порядке!')"
      ],
      "metadata": {
        "id": "Qk0Q4UPVozIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как использовать при парсинге:"
      ],
      "metadata": {
        "id": "vd-y0l-hp_EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unis_to_parse = ['hse', 'itmo', 'msu', 'hsе', 'spbu']"
      ],
      "metadata": {
        "id": "WarQ7vJ6pFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for address in unis_to_parse:\n",
        "    page_response = rq.get(f'https://{address}.ru')\n",
        "    current_html = BeautifulSoup(page_response.text)\n",
        "    print(current_html.title)"
      ],
      "metadata": {
        "id": "JUut8t2tpGeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for address in unis_to_parse:\n",
        "    try:\n",
        "        page_response = rq.get(f'https://{address}.ru')\n",
        "        current_html = BeautifulSoup(page_response.text)\n",
        "        print(current_html.title)\n",
        "    except:\n",
        "        print(f'С сайтом https://{address}.ru не работает')\n",
        "\n",
        "# hse - во втором случае использована е кириллицей"
      ],
      "metadata": {
        "id": "gUowN922pI1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF72_zk5ak4g"
      },
      "source": [
        "# Полезные ссылки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtTvDwFoqGox"
      },
      "source": [
        "Что почитать об использовании данных\n",
        "- [как устроен сбор данных](https://en.wikipedia.org/wiki/Web_scraping)\n",
        "\n",
        "\n",
        "- закон об авторском праве ([в деталях](http://www.consultant.ru/document/cons_doc_LAW_64629/0b318126c43879a845405f1fb1f4342f473a1eda/), [вкратце](https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D1%80%D1%81%D0%BA%D0%BE%D0%B5_%D0%BF%D1%80%D0%B0%D0%B2%D0%BE_%D0%B2_%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B8))\n",
        "- [закон о персональных данных](http://www.consultant.ru/document/cons_doc_LAW_61801/)\n",
        "- [типы лицензирования данных](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository)\n",
        "- [FAIR data](https://en.wikipedia.org/wiki/FAIR_data)\n",
        "- [OpenData](https://en.wikipedia.org/wiki/Open_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlEvq5ofqGox"
      },
      "source": [
        "Гайды и туториалы\n",
        "\n",
        "- [документация requests и быстрый гайд](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
        "\n",
        "\n",
        "- [документация Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "- [text-only](https://sjmulder.nl/en/textonly.html) веб-сайты, чтобы легко начать парсить\n",
        "\n",
        "\n",
        "\n",
        "- [здесь](https://www.york.ac.uk/teaching/cws/wws/webpage1.html) можно почитать про структуру html подробнее\n",
        "\n",
        "\n",
        "- [здесь](https://www.w3schools.com/html/html_examples.asp) еще и потренироваться в режиме онлайн (с этой ссылки мы начали занятие)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILp0Ke2daokq"
      },
      "source": [
        "Чем парсить соцсети (не исчерпывающий список)\n",
        "\n",
        "(стало сложнее, в России 1 и 2 ссылка не откроются без VPN...) \n",
        "\n",
        "- [Twitter](https://developer.twitter.com/en/docs/twitter-api/tools-and-libraries/v2)\n",
        "- [Meta](https://developers.facebook.com/docs/graph-api/)\n",
        "- [VK](https://vk-api.readthedocs.io/en/latest/), [положения о прайваси](https://vk.com/dev/uprivacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Посмотрите дополнительные тетрадки! \n",
        "\n",
        "[Здесь](https://github.com/AnnSenina/Python_for_CL/tree/main/notebooks/дополнительные%20тетрадки) появились новые тетрадки про скрейпинг"
      ],
      "metadata": {
        "id": "7oTag2sOt78g"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tQiBPzaI6F4y",
        "X5cEfTSEYhcY",
        "TjED1MGgqGot",
        "x3x43B4Qypk0",
        "bVR8OVuRUaO1",
        "yPczVAd5qGov",
        "OdOaCoseqGoy",
        "kwtSDxP_cLOX",
        "QQpr82aNkOOF",
        "rWlWI_6aoGPL",
        "tF72_zk5ak4g"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
