{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY4FeKDiytpz76YTavd21P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_for_CL/blob/main/notebooks/Python_11_n_gramms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Сначала спарсим текст и займемся его препроцессингом"
      ],
      "metadata": {
        "id": "se9a43F3m8Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если скачивать долго: готовые данные можно скачать [отсюда](https://www.kaggle.com/datasets/ashishsinhaiitr/lord-of-the-rings-text)\n",
        "\n",
        "Kaggle - требует регистрации, однако невероятно полезен!"
      ],
      "metadata": {
        "id": "pIcWNrygOwFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install requests \n",
        "# !pip install beautifulsoup4 \n",
        "\n",
        "import requests as rq\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "0C_SaUt8pRGs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://libcat.ru/knigi/fantastika-i-fjentezi/fentezi/278227-j-tolkien-the-lord-of-the-rings.html'\n",
        "page = rq.get(url) \n",
        "print(page) # посмотрим на код ответа, если 200, все хорошо"
      ],
      "metadata": {
        "id": "m9BU4Qe_o811"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.text, features=\"html.parser\") #сохраним результат в переменную soup\n",
        "print(soup.prettify()) # показывает нашу страницу в красивом виде"
      ],
      "metadata": {
        "id": "6Q0WKELwpg2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for i in soup.find_all(\"p\"):\n",
        "  text.append(i.text)\n",
        "\n",
        "text[8:-5]"
      ],
      "metadata": {
        "id": "tjQpQXjIp1Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links = ['https://libcat.ru/knigi/fantastika-i-fjentezi/fentezi/278227-j-tolkien-the-lord-of-the-rings.html']\n",
        "for i in range(2, 393):\n",
        "  link = 'https://libcat.ru/knigi/fantastika-i-fjentezi/fentezi/278227-' + str(i) + '-j-tolkien-the-lord-of-the-rings.html#text'\n",
        "  all_links.append(link)\n",
        "len(all_links)"
      ],
      "metadata": {
        "id": "szhlnvIoqSVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6d9cda-b195-4829-cdc6-1f8514e72950"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_links"
      ],
      "metadata": {
        "id": "uEXSm0aDrZM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Внимание! Парсить будем минут 5!\n",
        "\n",
        "Идеальное время для вопросов"
      ],
      "metadata": {
        "id": "gqGdSmMT9Yak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for i in all_links:\n",
        "  url = i\n",
        "  page = rq.get(url) \n",
        "  soup = BeautifulSoup(page.text, features=\"html.parser\") \n",
        "  for sent in soup.find_all(\"p\")[8:-5]:\n",
        "    text.append(sent.text)\n",
        "\n",
        "text\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "phVd2jCCrchL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_str = ' '.join(text)\n",
        "text_str\n",
        "#print(text_str)"
      ],
      "metadata": {
        "id": "LmemEVQGtLCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Препроцессим с NLTK"
      ],
      "metadata": {
        "id": "tL5psZ-Xtw2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btiSKDSom1Op"
      },
      "outputs": [],
      "source": [
        "# все то, что надо импортировать из NLTK\n",
        "#!pip install nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian') \n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "stemmer = SnowballStemmer(\"russian\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# собрала препроцессинг в 1 функцию - пока только до стемминга, лемматизация по плану в следующий раз\n",
        "def clean_text(text_str):\n",
        "  text_str = text_str.lower() # нижний регистр\n",
        "  text_list_nltk = word_tokenize(text_str) # токенизация\n",
        "  stop_words = stopwords.words('english') \n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalpha()] # удалить пунктуацию из списка токенов\n",
        "  text_clean = [word for word in text_without_punkt if word not in stop_words] # чистим от стоп-слов\n",
        "  text_stemmed = [stemmer.stem(word) for word in text_clean] # стемминг\n",
        "  return text_stemmed\n",
        "\n",
        "full_text = clean_text(text_str)"
      ],
      "metadata": {
        "id": "pESqIEADnaUT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text\n",
        "#print(full_text)"
      ],
      "metadata": {
        "id": "QDQs1sZbt5le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Готово! Вернемся к частотам"
      ],
      "metadata": {
        "id": "_J1yOC77vn00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(full_text).most_common(50)\n",
        "#print(Counter(full_text).most_common(50))"
      ],
      "metadata": {
        "id": "s96RXEpIvmTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вспомним, что не так с нашими частотами?"
      ],
      "metadata": {
        "id": "5smQhqxZwL1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Облако слов"
      ],
      "metadata": {
        "id": "4ppj8rIePYvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем инструменты для облака слов и списки стоп-слов\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "# Генерируем облако слов\n",
        "wordcloud = WordCloud().generate(', '.join(full_text))\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "ACDPjw0UPjbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width = 2000, \n",
        "                      height = 1500, \n",
        "                      background_color='black', \n",
        "                      colormap='Pastel1').generate(', '.join(full_text))\n",
        "plt.figure(figsize=(40, 30)) # Устанавливаем размер картинки\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "EVSRb4crYLU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание"
      ],
      "metadata": {
        "id": "5GbA2xl2KWEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cloud = '''В любой непонятной ситуации дата-сайентист визуализирует данные: это, среди прочего, облегчает поиск инсайтов и формулирование гипотез для проверки.\n",
        "Облако слов — визуализация текстовых данных на стыке исследовательского анализа, инфографики и дата-дизайна. Это самый первый и быстрый взгляд на большие и слабо структурированные тексты: художественные, научные, информационные.\n",
        "Главные причины использовать облако слов:\n",
        "Во-первых, это красиво — удачная визуализация украшает портфолио.\n",
        "Во-вторых, облако показывает самые популярные слова текста, что полезно для быстрой его оценки.\n",
        "Например, для школьного сочинения или текста в разговорном стиле это могут оказаться слова-паразиты (от таких неплохо бы избавляться), а для научных или «инфостильных» текстов — слова, больше относящиеся к содержанию.\n",
        "В-третьих, сделать такую визуализацию совсем не сложно — и сейчас вы сами в этом убедитесь.\n",
        "Мы будем работать в блокноте Google Colab — то есть прямо в браузере, код напишем на языке Python, а текст возьмём из «Википедии». Если что-то пойдёт не так — всегда можно свериться с нашим блокнотом: все ссылки есть в конце статьи.'''\n",
        "\n",
        "# текст отсюда https://skillbox.ru/media/code/vizualiziruy-eto-oblako-slov-na-python/"
      ],
      "metadata": {
        "id": "fnZDn_d6EZcE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n"
      ],
      "metadata": {
        "id": "RMddt4FyKyEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 2\n",
        "\n",
        "Сначала предобработайте этот текст, затем создайте облако слов"
      ],
      "metadata": {
        "id": "K0x4f9-YKxgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n"
      ],
      "metadata": {
        "id": "lluzrjCPKvGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Частотный анализ\n",
        "\n",
        "Многие компьтерные методы анализа текста основаны на статистике — в нашем случае это частотность символов / словоформ / лексем / биграмм / триграмм / частей речи и т.д., ее отношение к длине текста, средняя длина текстов и т.д.\n",
        "\n",
        "Зачем нам знать частотность слов в тексте? Например, она говорит о том, какие слова наиболее характеры для того или иного текста. Сравнивая частотные слова в разных текстах можно определить степень их близости, классифицировать по жанру, теме и т.п., а также выявить явления, характерные для языка в целом.\n",
        "\n",
        "Подход, когда текст представляется просто как куча слов, без информации об их порядке, называется bag of words."
      ],
      "metadata": {
        "id": "xnEdn25owVSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Закон Ципфа\n",
        "\n",
        "**Закон Ципфа** («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (т.н. рангу этого слова). Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и т.д.\n",
        "\n",
        "**Если закон Ципфа соблюдается — значит, перед нами нормальный текст на естественном языке. Если нет, то что-то с ним не так.**"
      ],
      "metadata": {
        "id": "shDhtULpxFXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Закон Хипса\n",
        "\n",
        "**Закон Хипса** — эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины. \n",
        "\n",
        "**Чем больше коллекция текстов, тем меньше новых токенов появляется с её пополнением**"
      ],
      "metadata": {
        "id": "yb2tiqDixNl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Способы считать частоту"
      ],
      "metadata": {
        "id": "2q_oo4IKxccT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Абсолютная частота слова\n",
        "Количество употреблений слова в тексте. Она не всегда уместна.\n"
      ],
      "metadata": {
        "id": "G2u_wc2Ixjz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Относительная частота слова\n",
        "это отношение его абсолютной частоты к какой-нибудь другой величине, например, к длине текста или корпуса. Существуют разные способы подсчета относительной частоты. "
      ],
      "metadata": {
        "id": "5lnQ_3m9xlLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IPM\n",
        "Для сравнения частот в разных коллекциях текстов популярен $ipm$ *(items per million)* - отношение абсолютной частоты какого-либо элемента к объему корпуса, умноженное на миллион.\n",
        "\n",
        "$$ ipm_{word} = \\dfrac{f_{word}}V_{corpus} \\        \\times \\  1,000,000 $$ \n",
        "\n",
        "Например, если текст состоит из 500 слов, и слово \"котик\" встречается там 50 раз, то \n",
        "\n",
        "$$ ipm_{котик} = \\dfrac{50}{500} \\       \\times \\  1,000,000 \\     = 100,000 $$ \n",
        "\n",
        "Метрика IPM позволяет сравнивать тексты через их характеристики. Например, \"Я\" заметно чаще встречается в корпусе любительской литературы, чем в корпусе художественных произведений из НКРЯ.\n",
        "\n",
        "Кстати, есть библиотеки с подсчетом ipm\n",
        "* [разные языки](https://pypi.org/project/wordfreq/)\n",
        "* отдельно [русский язык](https://pypi.org/project/ruword-frequency/)\n"
      ],
      "metadata": {
        "id": "AUwZBu5qxr4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordfreq"
      ],
      "metadata": {
        "id": "ALjNioev1GS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# как часто слово \"ring\" встречается в корпусе английского языка?\n",
        "from wordfreq import zipf_frequency\n",
        "ipm = zipf_frequency('ring', 'en')\n",
        "print(ipm)"
      ],
      "metadata": {
        "id": "O1pLsui91Mrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# а как часто - в нашем тексте?\n",
        "print(full_text.count(\"ring\") / len(full_text) * 1000000)"
      ],
      "metadata": {
        "id": "T1XrLQJDv8vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "Tf-Idf - способ высоко оценить слова, которые одновременно\n",
        "* показательны в документе\n",
        "* не вездесущи в корпусе документов\n",
        "\n",
        "\n",
        "Наивная идея такая: давайте оценка слова будет \n",
        "* увеличиваться, если оно частотно в документе\n",
        "* уменьшаться, если оно встречается во многих документах\n",
        "\n",
        "В таком противостоянии победят те слова, которые выделяют документы из многих им подобных.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tcK8TLJy3LSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Пример с использованием sklearn"
      ],
      "metadata": {
        "id": "-lGXkNlSq_t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала простой"
      ],
      "metadata": {
        "id": "u2M23xBQAJfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "XnIqgDLdAIhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['Three Rings for the Elven-kings under the sky',\n",
        "          'Seven for the Dwarf-lords in their halls of stone',\n",
        "          'Nine for Mortal Men doomed to die',\n",
        "          'One for the Dark Lord on his dark throne',\n",
        "          'In the Land of Mordor where the Shadows lie',\n",
        "          'One Ring to rule them all', \n",
        "          'One Ring to find them',\n",
        "          'One Ring to bring them all and in the darkness bind them',\n",
        "          'In the Land of Mordor where the Shadows lie']"
      ],
      "metadata": {
        "id": "KK9LpJ37AIbz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(tfidf)"
      ],
      "metadata": {
        "id": "ntFuCe7UAITA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.shape\n",
        "# предположите, что означают эти числа?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwrlQAYcBtYR",
        "outputId": "9b6c2958-c3b5-45fe-c475-638322bf399b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.todense() # матрица\n",
        "#print(tfidf.todense() # матрица)"
      ],
      "metadata": {
        "id": "WW2EIrkrA_Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer.vocabulary_ # словарь - не частотный!\n",
        "#print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "N5uqC6KwA_JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tfidf_vectorizer.get_feature_names_out()\n",
        "words\n",
        "#print(words)"
      ],
      "metadata": {
        "id": "7h8oISlZA_Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = tfidf.todense().tolist() \n",
        "lotr = pd.DataFrame(data, columns = words)\n",
        "lotr\n",
        "#print(lotr)"
      ],
      "metadata": {
        "id": "LBEfcocTBPqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сложный пример для нашего текста"
      ],
      "metadata": {
        "id": "uflf3TSkB0_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(text) # передаем текст как список строчек - матрица выстроится построчно\n",
        "# посмотрим на размер матрицы\n",
        "tfidf.shape"
      ],
      "metadata": {
        "id": "hSbOVBw_qVaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.todense() # матрица"
      ],
      "metadata": {
        "id": "8x7xb-5gqnHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer.vocabulary_ # словарь"
      ],
      "metadata": {
        "id": "Aun9Nrh0qec5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tfidf_vectorizer.get_feature_names_out()\n",
        "words"
      ],
      "metadata": {
        "id": "1V5pOVqcqhg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = tfidf.todense().tolist() \n",
        "df = pd.DataFrame(data, columns = words)"
      ],
      "metadata": {
        "id": "TOif6VxXqh1t"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df\n",
        "#print df"
      ],
      "metadata": {
        "id": "kic-AN7rWW6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df[\"frodo\"])"
      ],
      "metadata": {
        "id": "eWi_4WUzB9lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df[\"sam\"])"
      ],
      "metadata": {
        "id": "JjpjhiBZCKxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вернитесь на несколько шагов назад и постройте матрицу для первых 10 предложений\n",
        "\n",
        "(измените значение для метода .fit_transform())"
      ],
      "metadata": {
        "id": "j8kuO0GIDqqd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qQnufqFDy0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Доп. задание по tf-idf\n",
        "\n",
        "[Здесь](https://github.com/AnnSenina/Python_for_CL/tree/main/LotR) тексты отдельных книг из трилогии \"Властелин Колец\" \n",
        "\n",
        "Считайте их в три переменные и передайте в .fit_transform()) список из их названий"
      ],
      "metadata": {
        "id": "abxvyfV0E245"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34xJ63kbFTiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-gramms\n",
        "Текст можно разделить на n-граммы – устойчивые сочетания по N слов:\n",
        "\n",
        "    nltk.bigrams() – сочетания по два слова\n",
        "    nltk.trigrams() – сочетания по три слова\n",
        "    nltk.ngrams(list, n) – сочетания по N слов"
      ],
      "metadata": {
        "id": "rwkb1kYhXXry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "freq_trigramms = Counter(nltk.trigrams(full_text))\n",
        "freq_trigramms.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w59EOIs9XfpG",
        "outputId": "7fa277e8-ff10-4fcf-f6b7-831c96c41126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('mr.', 'frodo', 'said'), 43),\n",
              " (('let', 'us', 'go'), 36),\n",
              " (('sam', 'said', 'frodo'), 35),\n",
              " (('frodo', 'said', 'sam'), 21),\n",
              " (('aragorn', 'son', 'arathorn'), 20),\n",
              " (('gimli', 'glóin', 'son'), 13),\n",
              " (('yes', 'said', 'gandalf'), 12),\n",
              " (('could', 'see', 'nothing'), 12),\n",
              " (('yes', 'said', 'pippin'), 12),\n",
              " (('gimli', 'son', 'glóin'), 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}